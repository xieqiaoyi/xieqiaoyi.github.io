<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>强化学习--Reinforcement learning | Klein-bule</title><meta name="author" content="Qiaoyi Xie"><meta name="copyright" content="Qiaoyi Xie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="强化学习概念定义强化学习（Reinforcement learning，RL）讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的 环境(environment) 里面去极大化它能获得的奖励。通过感知所处环境的 状态(state) 对 动作(action) 的 **反应(reward)**， 来指导更好的动作，从而获得最大的 **收益(return)**，这被称为在交互中学习，这样的学习方">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习--Reinforcement learning">
<meta property="og:url" content="https://xieqiaoyi.github.io/2023/09/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Klein-bule">
<meta property="og:description" content="强化学习概念定义强化学习（Reinforcement learning，RL）讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的 环境(environment) 里面去极大化它能获得的奖励。通过感知所处环境的 状态(state) 对 动作(action) 的 **反应(reward)**， 来指导更好的动作，从而获得最大的 **收益(return)**，这被称为在交互中学习，这样的学习方">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xieqiaoyi.github.io/img/yileina.png">
<meta property="article:published_time" content="2023-09-27T14:25:00.055Z">
<meta property="article:modified_time" content="2023-09-27T14:26:21.225Z">
<meta property="article:author" content="Qiaoyi Xie">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xieqiaoyi.github.io/img/yileina.png"><link rel="shortcut icon" href="/img/yileina.png"><link rel="canonical" href="https://xieqiaoyi.github.io/2023/09/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '强化学习--Reinforcement learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-27 22:26:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/yileina.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Klein-bule"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">强化学习--Reinforcement learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-27T14:25:00.055Z" title="发表于 2023-09-27 22:25:00">2023-09-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-27T14:26:21.225Z" title="更新于 2023-09-27 22:26:21">2023-09-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="强化学习--Reinforcement learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><h3 id="概念定义"><a href="#概念定义" class="headerlink" title="概念定义"></a>概念定义</h3><p>强化学习（Reinforcement learning，RL）讨论的问题是一个<strong>智能体(agent)</strong> 怎么在一个复杂不确定的 <strong>环境(environment)</strong> 里面去极大化它能获得的奖励。通过感知所处环境的 <strong>状态(state)</strong> 对 <strong>动作(action)</strong> 的 **反应(reward)**， 来指导更好的动作，从而获得最大的 **收益(return)**，这被称为在交互中学习，这样的学习方法就被称作强化学习。</p>
<blockquote>
<p>Reinforcement learning is learning what to do—how to map situations to actions——so as to maximize a numerical reward signal.<br>—– Richard S. Sutton and Andrew G. Barto 《Reinforcement Learning: An Introduction II》</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202061348504.png"><img src="https://oss.imzhanghao.com/img/202202061348504.png" alt="强化学习"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202061348504.png">强化学习</a></p>
<p>在强化学习过程中，智能体跟环境一直在交互。智能体在环境里面获取到状态，智能体会利用这个状态输出一个动作，一个决策。然后这个决策会放到环境之中去，环境会根据智能体采取的决策，输出下一个状态以及当前的这个决策得到的奖励。智能体的目的就是为了尽可能多地从环境中获取奖励。</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080608917.png"><img src="https://oss.imzhanghao.com/img/202202080608917.png" alt="强化学习，监督学习，非监督学习"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080608917.png">强化学习，监督学习，非监督学习</a></p>
<p>强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。</p>
<ul>
<li><strong>监督学习</strong> 是从外部监督者提供的带标注训练集中进行学习。 <strong>(任务驱动型)</strong></li>
<li><strong>非监督学习</strong> 是一个典型的寻找未标注数据中隐含结构的过程。 <strong>(数据驱动型)</strong></li>
<li><strong>强化学习</strong> 更偏重于智能体与环境的交互， 这带来了一个独有的挑战 ——“<strong>试错（exploration）</strong>”与“<strong>开发（exploitation）</strong>”之间的折中权衡，智能体必须开发已有的经验来获取收益，同时也要进行试探，使得未来可以获得更好的动作选择空间。 <strong>(从错误中学习)</strong></li>
</ul>
<p>强化学习主要有以下几个特点：</p>
<ul>
<li><strong>试错学习</strong>：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。</li>
<li><strong>延迟回报</strong>：强化学习的指导信息很少，而且往往是在事后（最后一个状态(State)）才给出的。比如 围棋中只有到了最后才能知道胜负。</li>
</ul>
<h3 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080614963.png"><img src="https://oss.imzhanghao.com/img/202202080614963.png" alt="强化学习的两部分和三要素"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080614963.png">强化学习的两部分和三要素</a></p>
<ul>
<li><strong>环境(Environment)</strong> 是一个外部系统，智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。</li>
<li><strong>智能体(Agent)</strong> 是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态。</li>
<li>**状态(State)&#x2F;观察值(Observation)**：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息。</li>
<li>**动作(Action)**：不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces)，例如，走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间;如果机器人向 360◦ 中的任意角度都可以移动，则为连续动作空间。</li>
<li>**奖励(Reward)**：是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了智能体在某一步采 取了某个策略的表现如何。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071625676.png"><img src="https://oss.imzhanghao.com/img/202202071625676.png" alt="强化学习范例"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071625676.png">强化学习范例</a></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>对应上图中的内容</th>
</tr>
</thead>
<tbody><tr>
<td>agent</td>
<td>鸟</td>
</tr>
<tr>
<td>environment</td>
<td>鸟周围的环境，水管、天空（包括小鸟本身）</td>
</tr>
<tr>
<td>state</td>
<td>拍个照（目前的像素）</td>
</tr>
<tr>
<td>action</td>
<td>向上向下动作</td>
</tr>
<tr>
<td>reward</td>
<td>距离（越远奖励越高）</td>
</tr>
</tbody></table>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091739796.png"><img src="https://oss.imzhanghao.com/img/202202091739796.png" alt="强化学习应用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091739796.png">强化学习应用</a></p>
<p><strong>游戏</strong><br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080904190.png"><img src="https://oss.imzhanghao.com/img/202202080904190.png" alt="强化学习游戏领域的应用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080904190.png">强化学习游戏领域的应用</a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080905180.png"><img src="https://oss.imzhanghao.com/img/202202080905180.png" alt="强化学习游戏领域的应用">强化学习游戏领域的应用</a></p>
<p><strong>机器人</strong><br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080906945.png"><img src="https://oss.imzhanghao.com/img/202202080906945.png" alt="强化学习机器人领域的应用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080906945.png">强化学习机器人领域的应用</a></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>对应上图中的内容</th>
</tr>
</thead>
<tbody><tr>
<td>agent</td>
<td>策略-保持机器人平衡并行走</td>
</tr>
<tr>
<td>environment</td>
<td>机器人、地面、外部干扰</td>
</tr>
<tr>
<td>state</td>
<td>传感器采集的信号</td>
</tr>
<tr>
<td>action</td>
<td>作用在机器人各个关节的电机扭矩</td>
</tr>
<tr>
<td>reward</td>
<td>评估控制性能的数值信号</td>
</tr>
</tbody></table>
<p><strong>推荐广告</strong><br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080906718.png"><img src="https://oss.imzhanghao.com/img/202202080906718.png" alt="强化学习在推荐中的应用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080906718.png">强化学习在推荐中的应用</a></p>
<p>A Reinforcement Learning Framework for Explainable Recommendation</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080926942.png"><img src="https://oss.imzhanghao.com/img/202202080926942.png" alt="强化学习在广告中的应用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202080926942.png">强化学习在广告中的应用</a></p>
<p>Deep Reinforcement Learning for Online Advertising in Recommender Systems.<br>同时解决三个任务：是否插入广告；如果插入，插入哪一条广告；以及插入广告在推荐列表的哪个位置。</p>
<h3 id="相关术语"><a href="#相关术语" class="headerlink" title="相关术语"></a>相关术语</h3><p><strong>策略(Policy)</strong><br>策略是智能体用于决定下一步执行什么行动的规则。可以是确定性的，一般表示为：�μ:</p>
<p>��&#x3D;�(��)at&#x3D;μ(st)</p>
<p>也可以是随机的，一般表示为 �π:</p>
<p>��∼�(⋅∣��)at∼π(⋅∣st)</p>
<p><strong>状态转移(State Transition)</strong><br>状态转移，可以是确定的也可以是随机的，一般认为是随机的，其随机性来源于环境。可以用状态密度函数来表示：</p>
<p>�(�′∣�,�)&#x3D;�(�′&#x3D;�′∣�&#x3D;�,�&#x3D;�)p(s′∣s,a)&#x3D;P(S′&#x3D;s′∣S&#x3D;s,A&#x3D;a)</p>
<p>环境可能会变化，在当前环境和行动下，衡量系统状态向某一个状态转移的概率是多少，注意环境的变化通常是未知的。</p>
<p><strong>回报(Return)</strong><br>回报又称cumulated future reward，一般表示为�U，定义为</p>
<p>��&#x3D;��+��+1+��+2+��+3+⋯Ut&#x3D;Rt+Rt+1+Rt+2+Rt+3+⋯</p>
<p>其中��Rt表示第t时刻的奖励，agent的目标就是让Return最大化。</p>
<p>未来的奖励不如现在等值的奖励那么好（比如一年后给100块不如现在就给），所以��+1Rt+1的权重应该小于��Rt。因此，强化学习通常用discounted return（折扣回报，又称cumulative discounted future reward），取�γ为discount rate（折扣率），�∈(0,1]γ∈(0,1]，则有，</p>
<p>��&#x3D;��+���+1+�2��+2+�3��+3+⋯Ut&#x3D;Rt+γRt+1+γ2Rt+2+γ3Rt+3+⋯</p>
<p><strong>价值函数(Value Function)</strong><br>举例来说，在象棋游戏中，定义赢得游戏得1分，其他动作得0分，状态是棋盘上棋子的位置。仅从1分和0分这两个数值并不能知道智能体在游戏过程中到底下得怎么样，而通过价值函数则可以获得更多洞察。</p>
<p>价值函数使用期望对未来的收益进行预测，一方面不必等待未来的收益实际发生就可以获知当前状态的好坏，另一方面通过期望汇总了未来各种可能的收益情况。使用价值函数可以很方便地评价不同策略的好坏。</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071857303.png"><img src="https://oss.imzhanghao.com/img/202202071857303.png" alt="价值函数"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071857303.png">价值函数</a></p>
<ul>
<li>状态价值函数(State-value Function)：用来度量给定策略�π的情况下，当前状态��st的好坏程度。</li>
<li>动作价值函数(Action-value Function)：用来度量给定状态��st和策略�π的情况下，采用动作��at的好坏程度。</li>
</ul>
<h3 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071919418.png"><img src="https://oss.imzhanghao.com/img/202202071919418.png" alt="强化学习算法的分类"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202071919418.png">强化学习算法的分类</a></p>
<p><strong>按照环境是否已知划分：免模型学习（Model-Free） vs 有模型学习（Model-Based）</strong></p>
<ul>
<li><strong>Model-free</strong>就是不去学习和理解环境，环境给出什么信息就是什么信息，常见的方法有policy optimization和Q-learning。</li>
<li><strong>Model-Based</strong>是去学习和理解环境，学会用一个模型来模拟环境，通过模拟的环境来得到反馈。Model-Based相当于比Model-Free多了模拟环境这个环节，通过模拟环境预判接下来会发生的所有情况，然后选择最佳的情况。</li>
</ul>
<blockquote>
<p>一般情况下，环境都是不可知的，所以这里主要研究无模型问题。</p>
</blockquote>
<p><strong>按照学习方式划分：在线策略（On-Policy） vs 离线策略（Off-Policy）</strong></p>
<ul>
<li><strong>On-Policy</strong>是指agent必须本人在场， 并且一定是本人边玩边学习。典型的算法为Sarsa。</li>
<li><strong>Off-Policy</strong>是指agent可以选择自己玩， 也可以选择看着别人玩， 通过看别人玩来学习别人的行为准则， 离线学习同样是从过往的经验中学习， 但是这些过往的经历没必要是自己的经历， 任何人的经历都能被学习，也没有必要是边玩边学习，玩和学习的时间可以不同步。典型的方法是Q-learning，以及Deep-Q-Network。</li>
</ul>
<p><strong>按照学习目标划分：基于策略（Policy-Based）和基于价值（Value-Based）。</strong><br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081103918.png"><img src="https://oss.imzhanghao.com/img/202202081103918.png" alt="基于策略VS基于价值"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081103918.png">基于策略VS基于价值</a></p>
<ul>
<li><strong>Policy-Based</strong>的方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。</li>
<li><strong>Value-Based</strong>的方法输出的是动作的价值，选择价值最高的动作。适用于非连续的动作。常见的方法有Q-learning、Deep Q Network和Sarsa。</li>
<li>更为厉害的方法是二者的结合：Actor-Critic，Actor根据概率做出动作，Critic根据动作给出价值，从而加速学习过程，常见的有A2C，A3C，DDPG等。</li>
</ul>
<h3 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h3><p>经典算法：Q-learning，Sarsa，DQN，Policy Gradient，A3C，DDPG，PPO</p>
<table>
<thead>
<tr>
<th>学习方法</th>
<th>说明</th>
<th>经典算法</th>
</tr>
</thead>
<tbody><tr>
<td>基于价值(Value-Based)</td>
<td>通过价值选行为</td>
<td>Q Learning， Sarsa， Deep Q Network</td>
</tr>
<tr>
<td>基于策略(Policy-Based)</td>
<td>直接选最佳行为</td>
<td>Policy Gradients</td>
</tr>
<tr>
<td>基于模型(Model-Based)</td>
<td>想象环境并从中学习</td>
<td>Model based RL</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081412652.png"><img src="https://oss.imzhanghao.com/img/202202081412652.png" alt="Value-Based &amp; Policy-Based &amp; Actor Critic"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081412652.png">Value-Based &amp; Policy-Based &amp; Actor Critic</a></p>
<p>下面我们挑选一些有代表性的算法进行讲解：</p>
<ul>
<li>基于表格、没有神经网络参与的Q-Learning算法</li>
<li>基于价值(Value-Based)的Deep Q Network（DQN）算法</li>
<li>基于策略(Policy-Based)的Policy Gradient（PG）算法</li>
<li>结合了Value-Based和Policy-Based的Actor Critic算法。</li>
</ul>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>在Q-learning中，我们维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在当前状态S下可以采用动作A可以获得的未来收益的折现和。我们不断的迭代我们的Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081001064.png"><img src="https://oss.imzhanghao.com/img/202202081001064.png" alt="Q-Learning场景范例"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081001064.png">Q-Learning场景范例</a></p>
<p>假设机器人必须越过迷宫并到达终点。有地雷，机器人一次只能移动一个地砖。如果机器人踏上矿井，机器人就死了。机器人必须在尽可能短的时间内到达终点。<br>得分&#x2F;奖励系统如下：</p>
<ul>
<li>机器人在每一步都失去1点。这样做是为了使机器人采用最短路径并尽可能快地到达目标。</li>
<li>如果机器人踩到地雷，则点损失为100并且游戏结束。</li>
<li>如果机器人获得动力⚡️，它会获得1点。</li>
<li>如果机器人达到最终目标，则机器人获得100分。<br>现在，显而易见的问题是：我们如何训练机器人以最短的路径到达最终目标而不踩矿井？</li>
</ul>
<h3 id="Q值表"><a href="#Q值表" class="headerlink" title="Q值表"></a>Q值表</h3><p>Q值表(Q-Table)是一个简单查找表的名称，我们计算每个状态的最大预期未来奖励。基本上，这张表将指导我们在每个状态采取最佳行动。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081003156.png"><img src="https://oss.imzhanghao.com/img/202202081003156.png" alt="Q-Table"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081003156.png">Q-Table</a></p>
<h3 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h3><p>Q函数(Q-Function)即为上文提到的动作价值函数，他有两个输入：「状态」和「动作」。它将返回在该状态下执行该动作的未来奖励期望。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081018874.png"><img src="https://oss.imzhanghao.com/img/202202081018874.png" alt="Q函数"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081018874.png">Q函数</a></p>
<p>我们可以把Q函数视为一个在Q-Table上滚动的读取器，用于寻找与当前状态关联的行以及与动作关联的列。它会从相匹配的单元格中返回 Q 值。这就是未来奖励的期望。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081020345.png"><img src="https://oss.imzhanghao.com/img/202202081020345.png" alt="Q函数"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081020345.png">Q函数</a></p>
<p>在我们探索环境（environment）之前，Q-table 会给出相同的任意的设定值（大多数情况下是 0）。随着对环境的持续探索，这个 Q-table 会通过迭代地使用 Bellman 方程（动态规划方程）更新 Q(s，a) 来给出越来越好的近似。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081004537.png"><img src="https://oss.imzhanghao.com/img/202202081004537.png" alt="Q-learning实现"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081004537.png">Q-learning实现</a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081004219.png"><img src="https://oss.imzhanghao.com/img/202202081004219.png" alt="Q-learning的学习过程"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081004219.png">Q-learning的学习过程</a></p>
<p><strong>第1步：初始化Q值表</strong><br>我们将首先构建一个Q值表。有n列，其中n&#x3D;操作数。有m行，其中m&#x3D;状态数。我们将值初始化为0<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081005071.png"><img src="https://oss.imzhanghao.com/img/202202081005071.png" alt="初始化Q表"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081005071.png">初始化Q表</a></p>
<p><strong>步骤2和3：选择并执行操作</strong><br>这些步骤的组合在不确定的时间内完成。这意味着此步骤一直运行，直到我们停止训练，或者训练循环停止。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081006986.png"><img src="https://oss.imzhanghao.com/img/202202081006986.png" alt="选择并执行操作"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081006986.png">选择并执行操作</a></p>
<p>如果每个Q值都等于零，我们就需要权衡探索&#x2F;利用（exploration&#x2F;exploitation）的程度了，思路就是，在一开始，我们将使用 epsilon 贪婪策略：</p>
<ul>
<li>我们指定一个探索速率「epsilon」，一开始将它设定为 1。这个就是我们将随机采用的步长。在一开始，这个速率应该处于最大值，因为我们不知道 Q-table 中任何的值。这意味着，我们需要通过随机选择动作进行大量的探索。</li>
<li>生成一个随机数。如果这个数大于 epsilon，那么我们将会进行「利用」（这意味着我们在每一步利用已经知道的信息选择动作）。否则，我们将继续进行探索。</li>
<li>在刚开始训练 Q 函数时，我们必须有一个大的 epsilon。随着智能体对估算出的 Q 值更有把握，我们将逐渐减小 epsilon。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081037361.png"><img src="https://oss.imzhanghao.com/img/202202081037361.png" alt="权衡探索和利用"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081037361.png">权衡探索和利用</a></p>
<p><strong>步骤4和5：评估</strong><br>现在我们采取了行动并观察了结果和奖励。我们需要更新功能Q（s，a）：<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081024769.png"><img src="https://oss.imzhanghao.com/img/202202081024769.png" alt="更新Q表"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081024769.png">更新Q表</a></p>
<p>最后生成的Q表：<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081011238.png"><img src="https://oss.imzhanghao.com/img/202202081011238.png" alt="生成的Q表"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081011238.png">生成的Q表</a></p>
<h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h2><p>在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不现实，我们无法构建可以存储超大状态空间的Q_table。不过，在机器学习中， 有一种方法对这种事情很在行，那就是神经网络，可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样就没必要在表格中记录 Q 值，而是直接使用神经网络预测Q值<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081012123.png"><img src="https://oss.imzhanghao.com/img/202202081012123.png" alt="Deep Q Network"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081012123.png">Deep Q Network</a></p>
<h3 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h3><p>DQN利用Qlearning特点，目标策略与动作策略分离，学习时利用经验池储存的经验取batch更新Q。同时提高了样本的利用率，也打乱了样本状态相关性使其符合神经网络的使用特点。</p>
<h3 id="固定Q目标"><a href="#固定Q目标" class="headerlink" title="固定Q目标"></a>固定Q目标</h3><p>神经网络一般学习的是固定的目标，而Qlearning中Q同样为学习的变化量，变动太大不利于学习。所以DQN使Q在一段时间内保持不变，使神经网络更易于学习。</p>
<h3 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081335646.png"><img src="https://oss.imzhanghao.com/img/202202081335646.png" alt="DQN算法流程"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081335646.png">DQN算法流程</a></p>
<h3 id="主要问题"><a href="#主要问题" class="headerlink" title="主要问题"></a>主要问题</h3><ul>
<li>在估计值函数的时候一个任意小的变化可能导致对应动作被选择或者不被选择，这种不连续的变化是致使基于值函数的方法无法得到收敛保证的重要因素。</li>
<li>选择最大的Q值这样一个搜索过程在高纬度或者连续空间是非常困难的；</li>
<li>无法学习到随机策略，有些情况下随机策略往往是最优策略。</li>
</ul>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>前面我们介绍的Q-Learning和DQN都是基于价值的强化学习算法，在给定一个状态下，计算采取每个动作的价值，我们选择有最高Q值（在所有状态下最大的期望奖励）的行动。如果我们省略中间的步骤，即直接根据当前的状态来选择动作，也就引出了强化学习中的另一种很重要的算法，即策略梯度(Policy Gradient， PG)</p>
<p>策略梯度不通过误差反向传播，它通过观测信息选出一个行为直接进行反向传播，当然出人意料的是他并没有误差，而是利用reward奖励直接对选择行为的可能性进行增强和减弱，好的行为会被增加下一次被选中的概率，不好的行为会被减弱下次被选中的概率。</p>
<p>举例如下图所示：输入当前的状态，输出action的概率分布，选择概率最大的一个action作为要执行的操作。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081608856.png"><img src="https://oss.imzhanghao.com/img/202202081608856.png" alt="Policy Gradient"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081608856.png">Policy Gradient</a></p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点</strong></p>
<ul>
<li>连续的动作空间（或者高维空间）中更加高效；</li>
<li>可以实现随机化的策略；</li>
<li>某种情况下，价值函数可能比较难以计算，而策略函数较容易。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>通常收敛到局部最优而非全局最优</li>
<li>评估一个策略通常低效（这个过程可能慢，但是具有更高的可变性，其中也会出现很多并不有效的尝试，而且方差高）</li>
</ul>
<h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h3><p>蒙特卡罗策略梯度reinforce算法是策略梯度最简单的也是最经典的一个算法。<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081616628.png"><img src="https://oss.imzhanghao.com/img/202202081616628.png" alt="REINFORCE算法流程"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081616628.png">REINFORCE算法流程</a></p>
<h3 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081645188.png"><img src="https://oss.imzhanghao.com/img/202202081645188.png" alt="REINFORCE流程图"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202081645188.png">REINFORCE流程图</a></p>
<p>首先我们需要一个 policy model 来输出动作概率，输出动作概率后，我们 sample() 函数去得到一个具体的动作，然后跟环境交互过后，我们可以得到一整个回合的数据。拿到回合数据之后，我再去执行一下 learn() 函数，在 learn() 函数里面，我就可以拿这些数据去构造损失函数，扔给这个优化器去优化，去更新我的 policy model。</p>
<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h2><p>演员-评论家算法(Actor-Critic)是基于策略(Policy Based)和基于价值(Value Based)相结合的方法</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091122103.png"><img src="https://oss.imzhanghao.com/img/202202091122103.png" alt="演员-评论家算法"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091122103.png">演员-评论家算法</a></p>
<ul>
<li>演员(Actor)是指策略函数 ��(�∣�)πθ(a∣s)，即学习一个策略来得到尽量高的回报。</li>
<li>评论家(Critic)是指值函数 ��(�)Vπ(s)，对当前策略的值函数进行估计，即评估演员的好坏。</li>
<li>借助于价值函数，演员-评论家算法可以进行单步更新参数，不需要等到回合结束才进行更新。</li>
</ul>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>整体结构：<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091127132.png"><img src="https://oss.imzhanghao.com/img/202202091127132.png" alt="演员-评论家网络结构"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091127132.png">演员-评论家网络结构</a></p>
<p>Actor和Critic的网络结构：<br><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091128295.png"><img src="https://oss.imzhanghao.com/img/202202091128295.png" alt="Actor Critic network "></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091128295.png">Actor Critic network</a></p>
<h3 id="算法流程-3"><a href="#算法流程-3" class="headerlink" title="算法流程"></a>算法流程</h3><p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091125622.png"><img src="https://oss.imzhanghao.com/img/202202091125622.png" alt="Actor Critic Algorithm"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091125622.png">Actor Critic Algorithm</a></p>
<h3 id="问题和改进"><a href="#问题和改进" class="headerlink" title="问题和改进"></a>问题和改进</h3><p>Actor Critic 取决于 Critic 的价值判断， 但是 Critic 难收敛， 再加上 Actor 的更新， 就更难收敛，为了解决该问题又提出了 A3C 算法和 DDPG 算法。</p>
<p><strong>改进算法1：A3C</strong><br>异步的优势行动者评论家算法（Asynchronous Advantage Actor-Critic，A3C），相比Actor-Critic，A3C的优化主要有3点，分别是异步训练框架，网络结构优化，Critic评估点的优化。其中异步训练框架是最大的优化。</p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091452021.png"><img src="https://oss.imzhanghao.com/img/202202091452021.png" alt="A3C"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091452021.png">A3C</a></p>
<p><strong>改进算法2：DDPG</strong><br>深度确定性策略梯度(Deep Deterministic Policy Gradient，DDPG)，从DDPG这个名字看，它是由D（Deep）+D（Deterministic ）+ PG(Policy Gradient)组成。</p>
<ul>
<li>Deep 是因为用了神经网络；</li>
<li>Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；</li>
<li>Policy Gradient 代表的是它用到的是策略网络。REINFORCE 算法每隔一个 episode 就更新一次，但 DDPG 网络是每个 step 都会更新一次 policy 网络，也就是说它是一个单步更新的 policy 网络。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091454219.png"><img src="https://oss.imzhanghao.com/img/202202091454219.png" alt="DDPG"></a></p>
<p><a target="_blank" rel="noopener" href="https://oss.imzhanghao.com/img/202202091454219.png">DDPG</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://spinningup.readthedocs.io/zh_CN/latest/spinningup/rl_intro.html">强化学习介绍 &#x2F; OpenAI Spinning Up</a></li>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/">EasyRL &#x2F; datawhalechina</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/category/1254674.html">0084. 强化学习(19) &#x2F; 刘建平Pinard &#x2F; cnblogs</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/DRL">Deep Reinforcement Learning &#x2F; wangshusen</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习Reinforcement Learning &#x2F; 莫烦Python</a></li>
<li><a target="_blank" rel="noopener" href="https://www.davidsilver.uk/teaching/">UCL Course on RL &#x2F; David Silver &#x2F; 2015</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning &#x2F; David Silver &#x2F; 2013</a></li>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">Human-level control through deep reinforcement learning &#x2F; David Silver &#x2F; 2015</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/xz15873139854/article/details/108032932">一图看懂DQN(Deep Q-Network)深度强化学习算法 &#x2F; 薄荷-塘</a></li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf">Policy gradient methods for reinforcement learning with function approximation.
</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36494307">强化学习——策略梯度与Actor-Critic算法 &#x2F; 野风 &#x2F; 知乎</a></li>
<li><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v48/mniha16.pdf">Asynchronous Methods for Deep Reinforcement Learning &#x2F; Google DeepMind &#x2F; A3C</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://xieqiaoyi.github.io">Qiaoyi Xie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://xieqiaoyi.github.io/2023/09/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">https://xieqiaoyi.github.io/2023/09/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xieqiaoyi.github.io" target="_blank">Klein-bule</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/yileina.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/09/stream-java/" title="Java8 中通过 Stream 对列表操作的几种方法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Java8 中通过 Stream 对列表操作的几种方法</div></div></a></div><div class="next-post pull-right"><a href="/2023/09/23/conda/" title="conda指令使用指南"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">conda指令使用指南</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/yileina.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Qiaoyi Xie</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xieqiaoyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">概念定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="toc-number">1.2.</span> <span class="toc-text">基本元素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.3.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%9C%AF%E8%AF%AD"><span class="toc-number">1.4.</span> <span class="toc-text">相关术语</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.5.</span> <span class="toc-text">算法分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">经典算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning"><span class="toc-number">2.</span> <span class="toc-text">Q-Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%80%BC%E8%A1%A8"><span class="toc-number">2.1.</span> <span class="toc-text">Q值表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">Q函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">算法流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Q-Network"><span class="toc-number">3.</span> <span class="toc-text">Deep Q Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="toc-number">3.1.</span> <span class="toc-text">经验回放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9AQ%E7%9B%AE%E6%A0%87"><span class="toc-number">3.2.</span> <span class="toc-text">固定Q目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-1"><span class="toc-number">3.3.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E9%97%AE%E9%A2%98"><span class="toc-number">3.4.</span> <span class="toc-text">主要问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">4.</span> <span class="toc-text">Policy Gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">4.1.</span> <span class="toc-text">优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#REINFORCE"><span class="toc-number">4.2.</span> <span class="toc-text">REINFORCE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-2"><span class="toc-number">4.3.</span> <span class="toc-text">算法流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic"><span class="toc-number">5.</span> <span class="toc-text">Actor Critic</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-3"><span class="toc-number">5.2.</span> <span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%92%8C%E6%94%B9%E8%BF%9B"><span class="toc-number">5.3.</span> <span class="toc-text">问题和改进</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">6.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/16/UML%E8%AF%AD%E6%B3%95/" title="UML基础语法教程">UML基础语法教程</a><time datetime="2023-10-16T10:51:27.407Z" title="发表于 2023-10-16 18:51:27">2023-10-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/11/%E5%85%AB%E6%95%B0%E7%A0%81%E9%97%AE%E9%A2%98/" title="八数码问题-8puzzle">八数码问题-8puzzle</a><time datetime="2023-10-10T16:33:33.472Z" title="发表于 2023-10-11 00:33:33">2023-10-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/10/09/stream-java/" title="Java8 中通过 Stream 对列表操作的几种方法">Java8 中通过 Stream 对列表操作的几种方法</a><time datetime="2023-10-09T13:19:40.448Z" title="发表于 2023-10-09 21:19:40">2023-10-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="强化学习--Reinforcement learning">强化学习--Reinforcement learning</a><time datetime="2023-09-27T14:25:00.055Z" title="发表于 2023-09-27 22:25:00">2023-09-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/23/conda/" title="conda指令使用指南">conda指令使用指南</a><time datetime="2023-09-23T05:24:54.925Z" title="发表于 2023-09-23 13:24:54">2023-09-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Qiaoyi Xie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>